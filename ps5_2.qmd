---
title: "Problem Set 5"
author: "Zijing Zhao, Zac Shen"
date: "11/5/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  echo: true
  eval: true
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1: Zijing Zhao, zijingz
    - Partner 2: Zekai Shen, (写一下你的cnet id)
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: Zijing Zhao, Zekai Shen
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup
import csv
import re
import geopandas as gpd
import json
import us
import altair_saver as alt_saver

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```

\newpage

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
#| eval: false
def scrape_page(page_number):
  base_url = "https://oig.hhs.gov/fraud/enforcement/?page="
  url = base_url + str(page_number)
  response = requests.get(url)
  soup = BeautifulSoup(response.content, 'html.parser')
  # Extract the required data
  data = []
  # every information required is stored in the cards under 'usa-card_container'
  cards = soup.find_all('div', class_ = 'usa-card__container')
  for card in cards:
    title = 'NA'
    link = 'NA'
    date = 'NA'
    category = 'NA'
    # title and link are under the same tag
    title_tag = card.find('h2')
    if title_tag:
      a_tag = title_tag.find('a')
      if a_tag and 'href' in a_tag.attrs:
        title =a_tag.text.strip()
        # this website uses relevant path
        link = 'https://oig.hhs.gov'+a_tag['href']
    # find the date
    date_tag = card.find('span')
    if date_tag:
      date = date_tag.text.strip()
    # find the category
    ul_tag = card.find('ul')
    if ul_tag:
      li_tag = ul_tag.find('li')
      category = li_tag.text.strip()
    data.append([title,date,category,link])
  return data

# Loop through all pages and scrape data
all_data = []
for page in range(1, 481):
    page_data = scrape_page(page)
    all_data.extend(page_data)
    time.sleep(0.1)

print(all_data)

df = pd.DataFrame(all_data, columns = ['Title', 'Date', 'Category', 'Link'])
print(df.head())

# Save the DataFrame to a CSV file
df.to_csv('scraped_data.csv', index=False)
```

```{python}
df = pd.read_csv('scraped_data.csv')
print(df.head())
```

### 2. Crawling (PARTNER 1)

```{python}
#| eval: false
import logging
# Set up logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

df = pd.read_csv('scraped_data.csv')

def scrape_agency(link):
  response = requests.get(link)
  soup = BeautifulSoup(response.content, 'html.parser')
  
  # Extract the second li tag under the first ul tag
  agency = 'NA'
  # the Agency name locates next to the "Agency" span object
  span_tag = soup.find('span', text = 'Agency:')
  if span_tag:
    agency = span_tag.find_next_sibling(text=True).strip()
  logging.info(f'Finished scraping {link}')
  return agency
  

# Iterate through the DataFrame and scrape the agency data
df['Agency'] = df['Link'].apply(scrape_agency)

# Save the updated DataFrame to a CSV file
df.to_csv('scraped_data_2.csv', index=False)

# Print the updated DataFrame
print(df.head())
```

```{python}
df = pd.read_csv('scraped_data_2.csv')
print(df.head())
```



## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

def scrape_from(month,year):
  try year > 2013:
    for i in range(480):
      scrape_data()
        -----scraping codes for each page-----
        data.extend()
        if date < datetime(month+year):
          print('finish')
          break
  except:
    return ('please enter a date after Jan 1 2013')
  

* b. Create Dynamic Scraper (PARTNER 2)

```{python}
#| eval: false
# Function to scrape data from a given month and year
from datetime import datetime

# Function to scrape data from a given month and year
def scrape_from(month, year):
    if year < 2013:
        print("Please restrict to year >= 2013, since only enforcement actions after 2013 are listed.")
        return
    
    data = []
    page_number = 1
    while True:
        scraped_data = scrape_page(page_number)
        if not scraped_data:
            break
        data.extend(scraped_data)
        
        # Check if the date in the scraped data is before the given month and year
        last_date_str = scraped_data[-1]['Date']
        try:
            last_date = datetime.strptime(last_date_str, '%B %d, %Y')
        except ValueError:
            continue
        if last_date < datetime(year, month, 1):
            break
        page_number += 1
    
    df = pd.DataFrame(data)
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    filtered_df = df[df['Date'] >= datetime(year, month, 1)]
    
    # Scrape agency
    filtered_df['Agency'] = filtered_df['Link'].apply(scrape_agency)
    
    # Save the DataFrame to a CSV file
    file_name = f'enforcement_actions_{year}_{month}.csv'
    filtered_df.to_csv(file_name, index=False)
    
    return filtered_df

# Example usage
data_2023 = scrape_from(1, 2023)
print(len(data_2023))
print(data_2023.tail(1))
```

In this dataframe, 1534 enforcement were collected. The earliest enforcement acquired was Podiatrist Pays $90,000 To Settle False Billing Allegations on Jan 3, 2023.

* c. Test Partner's Code (PARTNER 1)

```{python}
#| eval: false
data_2021 = scrape_from(1, 2021)
print(len(data_2021))
print(data_2021.tail(1))
```

In this dataframe, 3022 enforcement were collected. The earliest enforcement acquired was The United States And Tennessee Resolve Claims With Three Providers For False Claims Act Liability Relating To ‘P-Stim’ Devices For A Total Of $1.72 Million on Jan 4, 2021.

\newpage

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)
Plot a line chart that shows: the number of enforcement actions over time (aggregated to each month+year) overall since January 2021

```{python}

```


### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python, eval=true}

```

## Step 4: Create maps of enforcement activity
For these questions, use this US Attorney District shapefile (link) and a Census state shapefile (link)

### 1. Map by State (PARTNER 1)
(Partner 1) Map by state: Among actions taken by state-level agencies, clean the state names you collected and plot a choropleth of the number of enforcement actions for each state. Hint: look for “State of” in the agency info!
```{python}


```


### 2. Map by District (PARTNER 2)
Map by district: Among actions taken by US Attorney District-level agencies, clean the district names so that you can merge them with the shapefile, and then plot a choropleth of the number of enforcement actions in each US Attorney District. Hint: look for “District” in the agency info.

```{python}

```


## Extra Credit
1. Use the zip code shapefile from the previous problem set and merge it with zip code-level population data. (Go to Census Data Portal, select “ZIP Code Tabulation Area”, check “All 5-digit ZIP Code Tabulation Areas within United States”, and select “2020: DEC Demographic and Housing Characteristics”. Download the csv.).
2. Conduct a spatial join between zip code shapefile and the district shapefile, then aggregate to get population in each district.
3. Map the ratio of enforcement actions in each US Attorney District. You can calculate the ratio by aggregating the number of enforcement actions since January 2021 per district, and dividing it with the population data.

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```